{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14455ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm  # For early_stopping callback\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_path = \"data/train.csv\"\n",
    "test_path = \"data/test.csv\"\n",
    "economic_indicators_path = \"data/EconomicIndicators.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "economic_data = pd.read_csv(economic_indicators_path)\n",
    "\n",
    "# Handle missing values\n",
    "train_data['InventoryRatio'].fillna(train_data['InventoryRatio'].median(), inplace=True)\n",
    "test_data['InventoryRatio'].fillna(test_data['InventoryRatio'].median(), inplace=True)\n",
    "\n",
    "# Convert Quarter to datetime\n",
    "def convert_quarter_to_datetime(df, quarter_col='Quarter', base_year=2020):\n",
    "    df['QuarterNum'] = df[quarter_col].str.extract(r'Q(\\d+)').astype(int)\n",
    "    df['Year'] = base_year + (df['QuarterNum'] - 1) // 4\n",
    "    df['Month'] = (3 * ((df['QuarterNum'] - 1) % 4)) + 1\n",
    "    df[quarter_col] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))\n",
    "    df.drop(columns=['QuarterNum', 'Year', 'Month'], inplace=True)\n",
    "    return df\n",
    "\n",
    "train_data = convert_quarter_to_datetime(train_data)\n",
    "test_data = convert_quarter_to_datetime(test_data)\n",
    "\n",
    "# Convert Economic Data Month to Quarter\n",
    "economic_data['Year'] = 2020 + (economic_data['Month'] - 1) // 12\n",
    "economic_data['Month'] = (economic_data['Month'] - 1) % 12 + 1\n",
    "economic_data['Date'] = pd.to_datetime(economic_data[['Year', 'Month']].assign(DAY=1))\n",
    "economic_data['Quarter'] = economic_data['Date'].dt.to_period('Q').astype(str)\n",
    "economic_quarterly = economic_data.groupby('Quarter').mean().reset_index()\n",
    "economic_quarterly['Year'] = economic_quarterly['Quarter'].str[:4].astype(int)\n",
    "economic_quarterly['Q'] = economic_quarterly['Quarter'].str[-1].astype(int)\n",
    "quarter_to_month = {1: 1, 2: 4, 3: 7, 4: 10}\n",
    "economic_quarterly['Month'] = economic_quarterly['Q'].map(quarter_to_month)\n",
    "economic_quarterly['Quarter'] = pd.to_datetime(economic_quarterly[['Year', 'Month']].assign(DAY=1))\n",
    "economic_quarterly.drop(columns=['Year', 'Q', 'Month', 'Date'], inplace=True, errors='ignore')\n",
    "\n",
    "# Merge with economic indicators\n",
    "train_data = train_data.merge(economic_quarterly, on='Quarter', how='left')\n",
    "test_data = test_data.merge(economic_quarterly, on='Quarter', how='left')\n",
    "\n",
    "# Feature Engineering\n",
    "# Lag Features: Previous Quarter Sales (1, 2, 3 quarters)\n",
    "train_data = train_data.sort_values(['Company', 'Quarter'])\n",
    "train_data['Prev_Quarter_Sales'] = train_data.groupby('Company')['Sales'].shift(1)\n",
    "train_data['Prev_2_Quarter_Sales'] = train_data.groupby('Company')['Sales'].shift(2)\n",
    "train_data['Prev_3_Quarter_Sales'] = train_data.groupby('Company')['Sales'].shift(3)\n",
    "train_data['Prev_Quarter_Sales'].fillna(train_data['Sales'].median(), inplace=True)\n",
    "train_data['Prev_2_Quarter_Sales'].fillna(train_data['Sales'].median(), inplace=True)\n",
    "train_data['Prev_3_Quarter_Sales'].fillna(train_data['Sales'].median(), inplace=True)\n",
    "latest_sales = train_data.groupby('Company')[['Sales']].last().reset_index()\n",
    "latest_sales.rename(columns={'Sales': 'Prev_Quarter_Sales'}, inplace=True)\n",
    "latest_sales_2 = train_data.groupby('Company')['Sales'].shift(1).groupby(train_data['Company']).last().reset_index()\n",
    "latest_sales_2.rename(columns={'Sales': 'Prev_2_Quarter_Sales'}, inplace=True)\n",
    "latest_sales_3 = train_data.groupby('Company')['Sales'].shift(2).groupby(train_data['Company']).last().reset_index()\n",
    "latest_sales_3.rename(columns={'Sales': 'Prev_3_Quarter_Sales'}, inplace=True)\n",
    "test_data = test_data.merge(latest_sales, on='Company', how='left')\n",
    "test_data = test_data.merge(latest_sales_2, on='Company', how='left')\n",
    "test_data = test_data.merge(latest_sales_3, on='Company', how='left')\n",
    "test_data['Prev_Quarter_Sales'].fillna(train_data['Sales'].median(), inplace=True)\n",
    "test_data['Prev_2_Quarter_Sales'].fillna(train_data['Sales'].median(), inplace=True)\n",
    "test_data['Prev_3_Quarter_Sales'].fillna(train_data['Sales'].median(), inplace=True)\n",
    "\n",
    "# Sales Trend Feature\n",
    "train_data['Sales_Trend'] = train_data['Prev_Quarter_Sales'] - train_data['Prev_2_Quarter_Sales']\n",
    "test_data['Sales_Trend'] = test_data['Prev_Quarter_Sales'] - test_data['Prev_2_Quarter_Sales']\n",
    "train_data['Sales_Trend'].fillna(0, inplace=True)\n",
    "test_data['Sales_Trend'].fillna(0, inplace=True)\n",
    "\n",
    "# New Interaction Features\n",
    "train_data['Sales_Trend_PMI'] = train_data['Sales_Trend'] * train_data['PMI']\n",
    "test_data['Sales_Trend_PMI'] = test_data['Sales_Trend'] * test_data['PMI']\n",
    "train_data['Sales_Trend_Interest'] = train_data['Sales_Trend'] * train_data['Interest Rate']\n",
    "test_data['Sales_Trend_Interest'] = test_data['Sales_Trend'] * test_data['Interest Rate']\n",
    "\n",
    "# Rolling Statistics for Sales\n",
    "train_data['Rolling_Avg_Sales'] = train_data.groupby('Company')['Sales'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "train_data['Rolling_Std_Sales'] = train_data.groupby('Company')['Sales'].transform(lambda x: x.rolling(3, min_periods=1).std())\n",
    "test_data = test_data.merge(train_data.groupby('Company')[['Rolling_Avg_Sales']].last().reset_index(), on='Company', how='left')\n",
    "test_data = test_data.merge(train_data.groupby('Company')[['Rolling_Std_Sales']].last().reset_index(), on='Company', how='left')\n",
    "train_data['Rolling_Std_Sales'].fillna(train_data['Rolling_Std_Sales'].median(), inplace=True)\n",
    "test_data['Rolling_Avg_Sales'].fillna(train_data['Rolling_Avg_Sales'].median(), inplace=True)\n",
    "test_data['Rolling_Std_Sales'].fillna(train_data['Rolling_Std_Sales'].median(), inplace=True)\n",
    "\n",
    "# Industry-Level Average Sales (using past data only)\n",
    "train_data = train_data.sort_values('Quarter')\n",
    "industry_avg_sales = train_data.groupby(['Industry', 'Quarter'])['Sales'].transform(lambda x: x.shift(1).rolling(2, min_periods=1).mean())\n",
    "train_data['Industry_Avg_Sales'] = industry_avg_sales\n",
    "train_data['Industry_Avg_Sales'].fillna(train_data['Industry_Avg_Sales'].median(), inplace=True)\n",
    "test_data = test_data.merge(train_data.groupby(['Industry', 'Quarter'])[['Industry_Avg_Sales']].last().reset_index(), on=['Industry', 'Quarter'], how='left')\n",
    "test_data['Industry_Avg_Sales'].fillna(train_data['Industry_Avg_Sales'].median(), inplace=True)\n",
    "\n",
    "# Historical Quarterly Sales Average (Seasonality)\n",
    "train_data['Quarter_Num'] = train_data['Quarter'].dt.quarter\n",
    "test_data['Quarter_Num'] = test_data['Quarter'].dt.quarter\n",
    "quarterly_avg_sales = train_data.groupby('Quarter_Num')['Sales'].mean().to_dict()\n",
    "train_data['Quarterly_Avg_Sales'] = train_data['Quarter_Num'].map(quarterly_avg_sales)\n",
    "test_data['Quarterly_Avg_Sales'] = test_data['Quarter_Num'].map(quarterly_avg_sales)\n",
    "\n",
    "# Interaction Features\n",
    "train_data['Rev_to_Inventory'] = train_data['RevenueGrowth'] / (train_data['InventoryRatio'] + 1e-5)\n",
    "test_data['Rev_to_Inventory'] = test_data['RevenueGrowth'] / (test_data['InventoryRatio'] + 1e-5)\n",
    "train_data['Market_Effectiveness'] = train_data['Marketshare'] / (train_data['RevenueGrowth'] + 1e-5)\n",
    "test_data['Market_Effectiveness'] = test_data['Marketshare'] / (test_data['RevenueGrowth'] + 1e-5)\n",
    "train_data['PMI_Rev_Interaction'] = train_data['PMI'] * train_data['RevenueGrowth']\n",
    "test_data['PMI_Rev_Interaction'] = test_data['PMI'] * test_data['RevenueGrowth']\n",
    "train_data['PMI_Inventory_Interaction'] = train_data['PMI'] * train_data['InventoryRatio']\n",
    "test_data['PMI_Inventory_Interaction'] = test_data['PMI'] * test_data['InventoryRatio']\n",
    "train_data['Interest_Market_Interaction'] = train_data['Interest Rate'] * train_data['Marketshare']\n",
    "test_data['Interest_Market_Interaction'] = test_data['Interest Rate'] * test_data['Marketshare']\n",
    "train_data['PMI_Prev_Sales'] = train_data['PMI'] * train_data['Prev_Quarter_Sales']\n",
    "test_data['PMI_Prev_Sales'] = test_data['PMI'] * test_data['Prev_Quarter_Sales']\n",
    "train_data['Interest_Prev_Sales'] = train_data['Interest Rate'] * train_data['Prev_Quarter_Sales']\n",
    "test_data['Interest_Prev_Sales'] = test_data['Interest Rate'] * test_data['Prev_Quarter_Sales']\n",
    "\n",
    "# Cyclical Encoding for Quarter\n",
    "train_data['Year'] = train_data['Quarter'].dt.year\n",
    "test_data['Year'] = test_data['Quarter'].dt.year\n",
    "train_data['Quarter_Sin'] = np.sin(2 * np.pi * train_data['Quarter_Num'] / 4)\n",
    "train_data['Quarter_Cos'] = np.cos(2 * np.pi * train_data['Quarter_Num'] / 4)\n",
    "test_data['Quarter_Sin'] = np.sin(2 * np.pi * test_data['Quarter_Num'] / 4)\n",
    "test_data['Quarter_Cos'] = np.cos(2 * np.pi * test_data['Quarter_Num'] / 4)\n",
    "\n",
    "# Cap Outliers in InventoryRatio\n",
    "train_data['InventoryRatio'] = train_data['InventoryRatio'].clip(upper=train_data['InventoryRatio'].quantile(0.95))\n",
    "test_data['InventoryRatio'] = test_data['InventoryRatio'].clip(upper=test_data['InventoryRatio'].quantile(0.95))\n",
    "\n",
    "# Encode Categorical Features\n",
    "categorical_columns = ['Company', 'Bond rating', 'Stock rating', 'Region', 'Industry']\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "    test_data[col] = test_data[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Drop Quarter column from both train and test data\n",
    "train_data.drop(columns=['Quarter'], inplace=True)\n",
    "test_data.drop(columns=['Quarter'], inplace=True)\n",
    "\n",
    "# Define Features\n",
    "features = ['Company', 'QuickRatio', 'InventoryRatio', 'RevenueGrowth', 'Marketshare',\n",
    "            'Bond rating', 'Stock rating', 'Region', 'Industry', 'Year', 'Quarter_Num',\n",
    "            'Rev_to_Inventory', 'Market_Effectiveness', 'Industry_Avg_Sales',\n",
    "            'Prev_Quarter_Sales', 'Prev_2_Quarter_Sales', 'Prev_3_Quarter_Sales',\n",
    "            'Rolling_Avg_Sales', 'Rolling_Std_Sales', 'PMI_Rev_Interaction',\n",
    "            'PMI_Inventory_Interaction', 'Interest_Market_Interaction',\n",
    "            'Quarter_Sin', 'Quarter_Cos', 'Consumer Sentiment', 'Interest Rate', 'PMI',\n",
    "            'Money Supply', 'NationalEAI', 'EastEAI', 'WestEAI', 'SouthEAI', 'NorthEAI',\n",
    "            'Sales_Trend', 'PMI_Prev_Sales', 'Interest_Prev_Sales',\n",
    "            'Sales_Trend_PMI', 'Sales_Trend_Interest', 'Quarterly_Avg_Sales']\n",
    "\n",
    "# Prepare Training and Validation Sets (Use Most Recent 20% for Validation)\n",
    "train_data = train_data.sort_values(['Company', 'Year', 'Quarter_Num'])  # Sort by Company and time\n",
    "n = len(train_data)\n",
    "train_size = int(0.8 * n)\n",
    "X = train_data[features]\n",
    "y = train_data['Sales']\n",
    "X_train = X.iloc[:train_size]\n",
    "y_train = y.iloc[:train_size]\n",
    "X_val = X.iloc[train_size:]\n",
    "y_val = y.iloc[train_size:]\n",
    "\n",
    "# Clean up memory\n",
    "gc.collect()\n",
    "\n",
    "# Model 1: RandomForest (Minimal Tuning, Likely to Be Dropped)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300, max_depth=5, min_samples_split=20, min_samples_leaf=8, random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "mae_rf_train = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "mae_rf_val = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "print(f\"Random Forest Train MAE: {mae_rf_train}\")\n",
    "print(f\"Random Forest Validation MAE: {mae_rf_val}\")\n",
    "\n",
    "# Model 2: XGBoost with Enhanced Hyperparameters\n",
    "xgb_params = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'subsample': [0.7, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.9],\n",
    "    'reg_lambda': [20, 30],  # Increased regularization\n",
    "    'reg_alpha': [10, 15],  # Increased regularization\n",
    "    'min_child_weight': [10, 15]  # Increased to control overfitting\n",
    "}\n",
    "xgb_model = XGBRegressor(random_state=42, early_stopping_rounds=30)  # Increased early stopping rounds\n",
    "xgb_search = RandomizedSearchCV(xgb_model, xgb_params, n_iter=10, cv=TimeSeriesSplit(n_splits=3), scoring='neg_mean_absolute_error', random_state=42)\n",
    "xgb_search.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "best_xgb_params = xgb_search.best_params_\n",
    "print(f\"Best XGBoost Parameters: {best_xgb_params}\")\n",
    "\n",
    "# Create a new XGBoost model without early_stopping_rounds for stacking\n",
    "best_xgb = XGBRegressor(**best_xgb_params, random_state=42)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Compute Train and Validation MAE for XGBoost\n",
    "y_train_pred_xgb = xgb_search.best_estimator_.predict(X_train)\n",
    "y_val_pred_xgb = xgb_search.best_estimator_.predict(X_val)\n",
    "mae_xgb_train = mean_absolute_error(y_train, y_train_pred_xgb)\n",
    "mae_xgb_val = mean_absolute_error(y_val, y_val_pred_xgb)\n",
    "print(f\"XGBoost Train MAE: {mae_xgb_train}\")\n",
    "print(f\"XGBoost Validation MAE: {mae_xgb_val}\")\n",
    "\n",
    "# Feature Importance Analysis (Drop Low-Impact Features)\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': best_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(\"\\nFeature Importances:\\n\", feature_importances)\n",
    "\n",
    "# Drop features with importance < 0.01\n",
    "low_impact_features = feature_importances[feature_importances['importance'] < 0.01]['feature'].tolist()\n",
    "print(f\"\\nDropping Low-Impact Features: {low_impact_features}\")\n",
    "features = [f for f in features if f not in low_impact_features]\n",
    "X_train = X_train[features]\n",
    "X_val = X_val[features]\n",
    "test_data = test_data[features + ['RowID']]  # Keep RowID for submission, but exclude during prediction\n",
    "\n",
    "# Retrain All Models on Reduced Feature Set\n",
    "# RandomForest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300, max_depth=5, min_samples_split=20, min_samples_leaf=8, random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "mae_rf_train = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "mae_rf_val = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "print(f\"Random Forest Train MAE (After Feature Selection): {mae_rf_train}\")\n",
    "print(f\"Random Forest Validation MAE (After Feature Selection): {mae_rf_val}\")\n",
    "\n",
    "# XGBoost\n",
    "best_xgb.fit(X_train, y_train)\n",
    "y_train_pred_xgb = best_xgb.predict(X_train)\n",
    "y_val_pred_xgb = best_xgb.predict(X_val)\n",
    "mae_xgb_train = mean_absolute_error(y_train, y_train_pred_xgb)\n",
    "mae_xgb_val = mean_absolute_error(y_val, y_val_pred_xgb)\n",
    "print(f\"XGBoost Train MAE (After Feature Selection): {mae_xgb_train}\")\n",
    "print(f\"XGBoost Validation MAE (After Feature Selection): {mae_xgb_val}\")\n",
    "\n",
    "# LightGBM\n",
    "lgbm_params = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, '0.03'],\n",
    "    'num_leaves': [15, 31],\n",
    "    'subsample': [0.7, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.9],\n",
    "    'min_data_in_leaf': [20, 50],\n",
    "    'lambda_l1': [0, 0.1],\n",
    "    'lambda_l2': [0, 0.1]\n",
    "}\n",
    "lgbm_model = LGBMRegressor(random_state=42)\n",
    "lgbm_search = RandomizedSearchCV(lgbm_model, lgbm_params, n_iter=10, cv=TimeSeriesSplit(n_splits=3), scoring='neg_mean_absolute_error', random_state=42)\n",
    "lgbm_search.fit(X_train, y_train)\n",
    "best_params = lgbm_search.best_params_\n",
    "print(f\"Best LightGBM Parameters: {best_params}\")\n",
    "\n",
    "best_lgbm = LGBMRegressor(**best_params, random_state=42)\n",
    "best_lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_metric='mae',\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lightgbm.early_stopping(stopping_rounds=10, verbose=False)]\n",
    ")\n",
    "\n",
    "y_train_pred_lgbm = best_lgbm.predict(X_train)\n",
    "y_val_pred_lgbm = best_lgbm.predict(X_val)\n",
    "mae_lgbm_train = mean_absolute_error(y_train, y_train_pred_lgbm)\n",
    "mae_lgbm_val = mean_absolute_error(y_val, y_val_pred_lgbm)\n",
    "print(f\"LightGBM Train MAE (After Feature Selection): {mae_lgbm_train}\")\n",
    "print(f\"LightGBM Validation MAE (After Feature Selection): {mae_lgbm_val}\")\n",
    "\n",
    "# Clean up memory\n",
    "del lgbm_model, lgbm_search\n",
    "gc.collect()\n",
    "\n",
    "# CatBoost\n",
    "catboost_params = {\n",
    "    'iterations': [200, 300],\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'l2_leaf_reg': [5, 7],\n",
    "    'subsample': [0.7, 0.9]\n",
    "}\n",
    "catboost_model = CatBoostRegressor(random_state=42, verbose=0)\n",
    "catboost_search = RandomizedSearchCV(catboost_model, catboost_params, n_iter=10, cv=TimeSeriesSplit(n_splits=3), scoring='neg_mean_absolute_error', random_state=42)\n",
    "catboost_search.fit(X_train, y_train)\n",
    "best_catboost = catboost_search.best_estimator_\n",
    "\n",
    "best_catboost.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=10, verbose=0)\n",
    "\n",
    "y_train_pred_catboost = best_catboost.predict(X_train)\n",
    "y_val_pred_catboost = best_catboost.predict(X_val)\n",
    "mae_catboost_train = mean_absolute_error(y_train, y_train_pred_catboost)\n",
    "mae_catboost_val = mean_absolute_error(y_val, y_val_pred_catboost)\n",
    "print(f\"CatBoost Train MAE (After Feature Selection): {mae_catboost_train}\")\n",
    "print(f\"CatBoost Validation MAE (After Feature Selection): {mae_catboost_val}\")\n",
    "\n",
    "# Clean up memory\n",
    "del catboost_model, catboost_search\n",
    "gc.collect()\n",
    "\n",
    "# Stacking Ensemble\n",
    "estimators = [\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', best_xgb),\n",
    "    ('lgbm', best_lgbm),\n",
    "    ('catboost', best_catboost)\n",
    "]\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_stack = stacking_model.predict(X_train)\n",
    "y_val_pred_stack = stacking_model.predict(X_val)\n",
    "mae_stack_train = mean_absolute_error(y_train, y_train_pred_stack)\n",
    "mae_stack_val = mean_absolute_error(y_val, y_val_pred_stack)\n",
    "print(f\"Stacking Ensemble Train MAE (After Feature Selection): {mae_stack_train}\")\n",
    "print(f\"Stacking Ensemble Validation MAE (After Feature Selection): {mae_stack_val}\")\n",
    "\n",
    "# Time-Series Cross-Validation for Stacking Model\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_maes = []\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_tr, X_v = X.iloc[train_idx][features], X.iloc[val_idx][features]\n",
    "    y_tr, y_v = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    stacking_model.fit(X_tr, y_tr)\n",
    "    y_v_pred = stacking_model.predict(X_v)\n",
    "    cv_maes.append(mean_absolute_error(y_v, y_v_pred))\n",
    "print(f\"Stacking Ensemble Time-Series CV MAE: {np.mean(cv_maes)} ± {np.std(cv_maes)}\")\n",
    "\n",
    "# Clean up memory\n",
    "gc.collect()\n",
    "\n",
    "# Optimize Ensemble Weights Using Grid Search\n",
    "weight_combinations = [\n",
    "    (w_rf, w_xgb, w_lgbm, w_catboost, w_stack)\n",
    "    for w_rf in [0.0, 0.05, 0.1]\n",
    "    for w_xgb in [0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    for w_lgbm in [0.2, 0.25, 0.3, 0.35]\n",
    "    for w_catboost in [0.05, 0.1, 0.15]\n",
    "    for w_stack in [0.1, 0.15, 0.2]\n",
    "    if abs(w_rf + w_xgb + w_lgbm + w_catboost + w_stack - 1.0) < 1e-5\n",
    "]\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_weights = None\n",
    "for weights in weight_combinations:\n",
    "    w_rf, w_xgb, w_lgbm, w_catboost, w_stack = weights\n",
    "    ensemble_pred = (w_rf * y_val_pred_rf + \n",
    "                     w_xgb * y_val_pred_xgb + \n",
    "                     w_lgbm * y_val_pred_lgbm + \n",
    "                     w_catboost * y_val_pred_catboost + \n",
    "                     w_stack * y_val_pred_stack)\n",
    "    mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_weights = {'rf': w_rf, 'xgb': w_xgb, 'lgbm': w_lgbm, 'catboost': w_catboost, 'stack': w_stack}\n",
    "\n",
    "print(f\"Optimized Ensemble Weights: {best_weights}\")\n",
    "print(f\"Ensemble Validation MAE with Optimized Weights: {best_mae}\")\n",
    "\n",
    "# Final Predictions with Optimized Weights\n",
    "test_pred_rf = rf_model.predict(test_data[features])\n",
    "test_pred_xgb = best_xgb.predict(test_data[features])\n",
    "test_pred_lgbm = best_lgbm.predict(test_data[features])\n",
    "test_pred_catboost = best_catboost.predict(test_data[features])\n",
    "test_pred_stack = stacking_model.predict(test_data[features])\n",
    "\n",
    "final_predictions = (\n",
    "    best_weights['rf'] * test_pred_rf +\n",
    "    best_weights['xgb'] * test_pred_xgb +\n",
    "    best_weights['lgbm'] * test_pred_lgbm +\n",
    "    best_weights['catboost'] * test_pred_catboost +\n",
    "    best_weights['stack'] * test_pred_stack\n",
    ")\n",
    "\n",
    "# Save Submission\n",
    "submission = pd.DataFrame({'RowID': test_data['RowID'], 'Sales': final_predictions})\n",
    "submission['Sales'] = submission['Sales'].round().astype(int)\n",
    "submission.to_csv(\"/Users/saitejasuppu/Downloads/submission_stacking_optimized_v10.csv\", index=False)\n",
    "print(\"✅ Predictions saved as submission_stacking_optimized_v10.csv\")\n",
    "\n",
    "# XGBoost-Only Submission\n",
    "xgb_predictions = best_xgb.predict(test_data[features])\n",
    "submission_xgb = pd.DataFrame({'RowID': test_data['RowID'], 'Sales': xgb_predictions})\n",
    "submission_xgb['Sales'] = submission_xgb['Sales'].round().astype(int)\n",
    "submission_xgb.to_csv(\"/Users/Downloads/submission_xgboost_optimized_v10.csv\", index=False)\n",
    "print(\"✅ XGBoost predictions saved as submission_xgboost_optimized_v10.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
